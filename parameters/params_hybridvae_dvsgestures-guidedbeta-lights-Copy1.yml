Nhid:
- 32
- 64
- 64
- 128
Mhid: []
inhib_layers:
- 40
- 40
- 40
cls_sq_layers:
- 40
- 40
- 40
return_meta: True # for dvsgesutres, return light and user data
time_shuffle: True # Shuffle the data being presented in the time.
alpha:
- 0.97
alpharp:
- 0.65
batch_size: 20 #20
beta:
- 0.92
betas:
- 0.
- 0.95
resume_from: None # resume from latest checkpoint of the specified model logfile. If you don't want to resume specify None
device: cuda:0
vae_beta: 1.2 # 1.2
dimz: 100 #20 #32 #64
num_classes: 4 # 3 if using right
num_augs: 1 # number of times to augment the training samples to increase # of samples - 1, if 1 then use default data only
class_weight: 1 # weight given to the losses of the classifiers, including both inhibitory losses
is_guided: 1 # use guided vae method, 1 is True, 0 is False
start_epoch: 0
use_aug: 1 # use augmented data, 1 is True, 0 is false
burnin_steps: 100
chunk_size_test: 200
chunk_size_train: 200
dataset_dir: ../data/
dataset: torchneuromorphic.dvs_gestures.dvsgestures_dataloaders #userbal_noaug 
deltat: 1000
input_shape:
- 2
- 32
- 32
output_shape:
- 2
- 32
- 32
kernel_size:
- 5
lc_ampl: 0.5
learning_rate: 
- .00001 #0.0003 # encoder learning rate. # DO NOT HAVE AT 0.01 OR ABOVE OR IT GOES NAN
- .00001 #.0003 # this learning rate is for encoder_head, cls_sq, and decoder
- .00001 #0.0001 # now it's actually used, cls_sq learning rate (excitation net)
- .00001 #0.0001 inhib net learning rate # divergence for second loss is bad?
learning_method: 'bptt'
loss: smoothL1
lr_drop_factor: 1.01
lr_drop_interval: 100
num_epochs: 201 #14901
num_conv_layers: 4
num_mlp_layers: 0
num_layers: 4
num_dl_workers: 8
optimizer: adamax
out_channels: 10 #11
online_update: False
pool_size:
- 2
- 1
- 2
- 1
- 1
random_tau: false
reg_l:
- .0
- .0
- .0
- .0
stride:
- 1
- 1
- 1
- 1
test_interval: 10 #10
